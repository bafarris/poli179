{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### WORK IN PROGRESS!!!"
      ],
      "metadata": {
        "id": "aShmZc7aMZZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applying Bi-LSTM Model to Political Speech - Predicting Sentiment Scores (Sample Data)\n",
        "### POLI 179 Project - Brenna Farris and Eden Stewart\n",
        "\n",
        "Further examining this research: https://doi.org/10.1080/10584609.2021.1952497\n",
        "\n",
        "Data accessed from: https://github.com/ccochrane/emotionTranscripts"
      ],
      "metadata": {
        "id": "vIjJ6baqBU1H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Libraries and Dataset(s)"
      ],
      "metadata": {
        "id": "k4Av_CxnCXyD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8vTaZMaBR3_",
        "outputId": "d0b45a07-4175-4b89-e8a7-736f5b0397a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# import libraries and packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from zipfile import ZipFile\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Bidirectional, Dense, Embedding\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset containing human coder sentiment scores and sections of corpus with assigned sentiment scores\n",
        "full_df = pd.read_csv(\n",
        "    'https://raw.githubusercontent.com/bafarris/speech-sentiment-bilstm/main/data/w2vScores.csv',\n",
        "    sep=','\n",
        ")\n",
        "\n",
        "# examine human coder dataset\n",
        "print(full_df.columns)\n",
        "print(full_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8YRgsSgBf9F",
        "outputId": "96c944c9-9b2f-41f3-82d6-0bacfdcda634"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Unnamed: 0', 'IDMain', 'countedWords', 'date', 'english', 'floor',\n",
            "       'french', 'label', 'party', 'seconds', 'sentencePolarity', 'sentiment',\n",
            "       'speaker', 'timeStamp', 'wordPolaritySummed', 'youTube'],\n",
            "      dtype='object')\n",
            "   Unnamed: 0        IDMain  countedWords        date  \\\n",
            "0           0  2017 12 13 0            13  2017-12-13   \n",
            "1           1  2017 12 13 1             8  2017-12-13   \n",
            "2           2  2017 12 13 2            16  2017-12-13   \n",
            "3           3  2017 12 13 3            30  2017-12-13   \n",
            "4           4  2017 12 13 4             7  2017-12-13   \n",
            "\n",
            "                                             english  \\\n",
            "0  I thought we usually hired an investigator to ...   \n",
            "1  I hope they will be getting better than that i...   \n",
            "2  We told Canadians that we would run deficits, ...   \n",
            "3  The problem was that they fired 700 people in ...   \n",
            "4  We did not create the Phoenix problem, but we ...   \n",
            "\n",
            "                                               floor  french  label party  \\\n",
            "0  Moi, je pensais qu'on embauchait d'habitude un...       1      1   NDP   \n",
            "1  I hope they will be getting better than that i...       0      2   LIB   \n",
            "2  We told Canadians that we would run deficits, ...       0      3   LIB   \n",
            "3  The problem was that they fired 700 people in ...       0      4   LIB   \n",
            "4  We did not create the Phoenix problem, but we ...       0      5   LIB   \n",
            "\n",
            "  seconds  sentencePolarity  sentiment              speaker timeStamp  \\\n",
            "0    6.45          0.424467  -0.424467  Alexandre Boulerice    9M 12S   \n",
            "1    3.48          0.240783   0.240783       Justin Trudeau   11M 14S   \n",
            "2     7.5          0.127200   0.127200       Justin Trudeau   13M 41S   \n",
            "3   21.46          1.025146  -1.025146       Justin Trudeau   14M 33S   \n",
            "4    5.25          0.199474  -0.199474       Justin Trudeau    15M 9S   \n",
            "\n",
            "   wordPolaritySummed                       youTube  \n",
            "0            0.681408  https://youtu.be/6p2IWa2rfO4  \n",
            "1            0.408897  https://youtu.be/k-r7cKCu1bI  \n",
            "2            0.746691  https://youtu.be/zUqZsnLbFlc  \n",
            "3            1.611323  https://youtu.be/O_f9JztOy18  \n",
            "4            0.411969  https://youtu.be/d2ZUaGqybPc  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-Processing"
      ],
      "metadata": {
        "id": "Zp9eRyZ_CRZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load and define stop words for preprocessing\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# tokenize\n",
        "def preprocess_text(text):\n",
        "    words = word_tokenize(text.lower())\n",
        "    words = [word for word in words if word.isalnum() and word not in stop_words]\n",
        "    return words"
      ],
      "metadata": {
        "id": "eWzJaCWTCcOz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply preprocessing to the 'speech' column\n",
        "full_df['tokens'] = full_df['english'].fillna('').apply(preprocess_text)"
      ],
      "metadata": {
        "id": "U0Lu2sUTCqmK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare sentences for word2vec model training\n",
        "sentences = full_df['tokens'].tolist()"
      ],
      "metadata": {
        "id": "dqHYBGIFCzr7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train word2vec model\n",
        "word2vec_model_sample = Word2Vec(sentences, vector_size=300, window=6, min_count=10, epochs=5)\n",
        "word2vec_model_sample.save(\"word2vec_sample.model\")\n",
        "\n",
        "# print model vocabulary\n",
        "print(word2vec_model_sample.wv.key_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9S2hwfhUC6f0",
        "outputId": "e5564347-7b3b-4978-963d-f4719706331d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'minister': 0, 'speaker': 1, 'government': 2, 'canadians': 3, 'canada': 4, 'prime': 5, 'canadian': 6, 'work': 7, 'would': 8, 'liberals': 9, 'house': 10, 'people': 11, 'jobs': 12, 'new': 13, 'member': 14, 'us': 15, 'tax': 16, 'support': 17, 'members': 18, 'public': 19, 'plan': 20, 'going': 21, 'like': 22, 'get': 23, 'make': 24, 'know': 25, 'liberal': 26, 'one': 27, 'help': 28, 'working': 29, 'families': 30, 'said': 31, 'conservatives': 32, 'communities': 33, 'put': 34, 'national': 35, 'quebec': 36, 'also': 37, 'process': 38, 'country': 39, 'health': 40, 'years': 41, 'finance': 42, 'continue': 43, 'important': 44, 'year': 45, 'bill': 46, 'ensure': 47, 'take': 48, 'every': 49, 'commissioner': 50, 'need': 51, 'money': 52, 'across': 53, 'yesterday': 54, 'million': 55, 'system': 56, 'economy': 57, 'access': 58, 'ndp': 59, 'want': 60, 'last': 61, 'trade': 62, 'question': 63, 'conservative': 64, 'future': 65, 'many': 66, 'budget': 67, 'way': 68, 'women': 69, 'first': 70, 'indigenous': 71, 'making': 72, 'clear': 73, 'week': 74, 'could': 75, 'good': 76, 'today': 77, 'taxes': 78, 'create': 79, 'action': 80, 'even': 81, 'made': 82, 'party': 83, 'safety': 84, 'time': 85, 'interest': 86, 'information': 87, 'right': 88, 'actually': 89, 'fact': 90, 'go': 91, 'care': 92, 'order': 93, 'provinces': 94, 'infrastructure': 95, 'since': 96, 'pay': 97, 'well': 98, 'see': 99, 'done': 100, 'previous': 101, 'report': 102, 'committee': 103, 'decision': 104, 'two': 105, 'understand': 106, 'act': 107, 'number': 108, 'sure': 109, 'industry': 110, 'back': 111, 'place': 112, 'whether': 113, 'bring': 114, 'agreement': 115, 'ethics': 116, 'parliament': 117, 'rules': 118, 'open': 119, 'conflict': 120, 'former': 121, 'class': 122, 'stop': 123, 'tell': 124, '10': 125, 'federal': 126, 'giving': 127, 'review': 128, 'job': 129, 'deal': 130, 'growth': 131, 'protect': 132, 'security': 133, 'use': 134, 'together': 135, 'thing': 136, 'sector': 137, 'forward': 138, 'coast': 139, 'business': 140, 'law': 141, 'believe': 142, 'respect': 143, 'promised': 144, 'say': 145, 'senate': 146, 'let': 147, 'billion': 148, 'workers': 149, 'committed': 150, 'children': 151, 'issue': 152, 'riding': 153, 'middle': 154, 'still': 155, 'needs': 156, 'energy': 157, 'small': 158, 'states': 159, 'political': 160, 'part': 161, 'nations': 162, 'measures': 163, 'dollars': 164, 'however': 165, 'strong': 166, 'stand': 167, 'international': 168, 'benefits': 169, 'projects': 170, 'colleague': 171, 'hon': 172, 'side': 173, 'taken': 174, 'proud': 175, 'full': 176, 'already': 177, 'answer': 178, 'meet': 179, 'current': 180, 'hard': 181, 'better': 182, 'investments': 183, 'funding': 184, 'world': 185, 'opposite': 186, 'foreign': 187, 'court': 188, 'board': 189, 'instead': 190, 'rights': 191, 'finally': 192, 'exactly': 193, 'seniors': 194, 'creating': 195, 'end': 196, 'benefit': 197, 'child': 198, 'thank': 199, 'encourage': 200, 'given': 201, 'defence': 202, 'meeting': 203, 'peoples': 204, 'environment': 205, 'confirm': 206, 'taking': 207, 'united': 208, 'economic': 209, 'office': 210, 'community': 211, 'opposition': 212, 'approach': 213, 'farmers': 214, 'things': 215, 'allow': 216, 'leader': 217, 'terms': 218, 'income': 219}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply Bi-LSTM Model"
      ],
      "metadata": {
        "id": "eRIMQLZoDnxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert tokens to sequences of integers\n",
        "def tokens_to_sequence(tokens, word2vec_model):\n",
        "    return [word2vec_model.wv.key_to_index[word] for word in tokens if word in word2vec_model.wv.key_to_index]\n",
        "\n",
        "full_df['sequence'] = full_df['tokens'].apply(lambda x: tokens_to_sequence(x, word2vec_model_sample))"
      ],
      "metadata": {
        "id": "n0POhPhBDnaj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create training and test sets\n",
        "\n",
        "X = pad_sequences(full_df['sequence'], maxlen=100)  # assuming max length of sequences is 100\n",
        "y = full_df['sentiment']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "wlY3tJPpEWor"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build the Bi-LSTM Model\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(word2vec_model_sample.wv), output_dim=300, weights=[word2vec_model_sample.wv.vectors], input_length=100, trainable=False))\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add(Dense(1, activation='linear'))  # linear activation for regression task\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])"
      ],
      "metadata": {
        "id": "lCPAo7_oEZP4"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}